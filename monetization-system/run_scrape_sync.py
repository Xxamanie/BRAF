#!/usr/bin/env python3
"""
Production Web Scraper Runner (Synchronous Version)
Uses synchronous Playwright for better reliability and simpler error handling.
"""
import logging
import sys
import os
import json
import traceback
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import signal
import time
from urllib.parse import urlparse
import hashlib

# Configuration
LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
LOG_FILE = os.path.join(LOG_DIR, 'scraper.log')
ERROR_LOG_FILE = os.path.join(LOG_DIR, 'scraper_errors.log')
STATUS_FILE = os.path.join(os.path.dirname(__file__), 'data', 'scraper_status.json')

# Default URLs to scrape (can be overridden via config file)
DEFAULT_URLS = [
    'https://news.ycombinator.com',
    'https://httpbin.org/html',
    'https://example.com',
    'https://www.reddit.com/r/programming.json',
]

# Scraping configuration
SCRAPE_CONFIG = {
    'max_pages_per_run': 10,
    'delay_between_pages': 2,  # seconds
    'timeout_per_page': 30,    # seconds
    'max_retries': 3,
    'headless': True,
    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

class SyncScraperRunner:
    """Synchronous scraper runner with comprehensive error handling and logging"""
    
    def __init__(self):
        self.setup_logging()
        self.setup_directories()
        self.logger = logging.getLogger(__name__)
        self.start_time = datetime.now()
        self.stats = {
            'pages_scraped': 0,
            'pages_failed': 0,
            'pages_skipped': 0,
            'total_content_length': 0,
            'errors': []
        }
        
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        self.shutdown_requested = False
    
    def setup_logging(self):
        """Setup comprehensive logging configuration"""
        os.makedirs(LOG_DIR, exist_ok=True)
        
        # Main logger configuration
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(LOG_FILE, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        # Error logger for critical issues
        error_handler = logging.FileHandler(ERROR_LOG_FILE, encoding='utf-8')
        error_handler.setLevel(logging.ERROR)
        error_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s\n%(pathname)s:%(lineno)d\n%(funcName)s\n'
        )
        error_handler.setFormatter(error_formatter)
        
        # Add error handler to root logger
        logging.getLogger().addHandler(error_handler)
    
    def setup_directories(self):
        """Ensure all required directories exist"""
        directories = [
            LOG_DIR,
            os.path.join(os.path.dirname(__file__), 'data'),
            os.path.join(os.path.dirname(__file__), 'backups')
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully"""
        self.logger.warning(f"Received signal {signum}, initiating graceful shutdown...")
        self.shutdown_requested = True
    
    def load_urls_config(self) -> List[str]:
        """Load URLs from configuration file or use defaults"""
        config_file = os.path.join(os.path.dirname(__file__), 'scraper_urls.json')
        
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r') as f:
                    config = json.load(f)
                    urls = config.get('urls', DEFAULT_URLS)
                    self.logger.info(f"Loaded {len(urls)} URLs from config file")
                    return urls
        except Exception as e:
            self.logger.warning(f"Failed to load URL config: {e}, using defaults")
        
        return DEFAULT_URLS
    
    def save_status(self, status: str, details: Dict = None):
        """Save current scraper status to file"""
        try:
            status_data = {
                'status': status,
                'timestamp': datetime.now().isoformat(),
                'stats': self.stats,
                'details': details or {}
            }
            
            with open(STATUS_FILE, 'w') as f:
                json.dump(status_data, f, indent=2)
                
        except Exception as e:
            self.logger.error(f"Failed to save status: {e}")
    
    def initialize_scraper(self):
        """Initialize the synchronous scraper"""
        try:
            from sync_playwright_scraper import SyncPlaywrightScraper
            self.scraper = SyncPlaywrightScraper(headless=SCRAPE_CONFIG['headless'])
            self.logger.info("âœ… Sync Playwright scraper initialized")
            return True
            
        except ImportError as e:
            self.logger.error(f"âŒ Failed to initialize sync scraper: {e}")
            return False
    
    def scrape_single_url(self, url: str) -> tuple[bool, Dict]:
        """Scrape a single URL with error handling and retries"""
        for attempt in range(SCRAPE_CONFIG['max_retries']):
            if self.shutdown_requested:
                return False, {'error': 'Shutdown requested'}
            
            try:
                self.logger.info(f"ğŸ“¥ Scraping {url} (attempt {attempt + 1})")
                
                target = {"url": url}
                result = self.scraper.run_single_scrape(target)
                
                if result.success:
                    self.stats['pages_scraped'] += 1
                    self.stats['total_content_length'] += len(result.content)
                    return True, {
                        'url': url,
                        'title': result.title,
                        'content_length': len(result.content),
                        'word_count': result.word_count,
                        'domain': result.domain
                    }
                else:
                    # Failed scrape, will retry
                    error_msg = result.error or "Unknown error"
                    if attempt == SCRAPE_CONFIG['max_retries'] - 1:
                        self.stats['errors'].append(f"{url}: {error_msg}")
                        return False, {'error': error_msg}
                    
                    # Wait before retry
                    if attempt < SCRAPE_CONFIG['max_retries'] - 1:
                        time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                
            except Exception as e:
                error_msg = f"Error scraping {url} (attempt {attempt + 1}): {e}"
                self.logger.error(error_msg, exc_info=True)
                if attempt == SCRAPE_CONFIG['max_retries'] - 1:
                    self.stats['errors'].append(error_msg)
                    return False, {'error': str(e)}
                
                # Wait before retry
                if attempt < SCRAPE_CONFIG['max_retries'] - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
        
        return False, {'error': 'Max retries exceeded'}
    
    def run_scraping_session(self) -> bool:
        """Run a complete scraping session"""
        self.logger.info("ğŸš€ Starting scheduled scrape job")
        self.save_status('running', {'start_time': self.start_time.isoformat()})
        
        try:
            # Initialize scraper
            if not self.initialize_scraper():
                self.logger.error("âŒ Failed to initialize scraper")
                self.save_status('failed', {'error': 'Scraper initialization failed'})
                return False
            
            # Load URLs to scrape
            urls = self.load_urls_config()
            urls = urls[:SCRAPE_CONFIG['max_pages_per_run']]  # Limit pages per run
            
            self.logger.info(f"ğŸ“‹ Loaded {len(urls)} URLs to scrape")
            
            # Scrape each URL
            results = []
            for i, url in enumerate(urls):
                if self.shutdown_requested:
                    self.logger.info("ğŸ›‘ Shutdown requested, stopping scraping")
                    break
                
                success, result = self.scrape_single_url(url)
                results.append({
                    'url': url,
                    'success': success,
                    'result': result
                })
                
                if success:
                    self.logger.info(f"âœ… Successfully scraped {url}")
                else:
                    self.logger.warning(f"âŒ Failed to scrape {url}: {result.get('error', 'Unknown error')}")
                    self.stats['pages_failed'] += 1
                
                # Delay between pages to be respectful
                if i < len(urls) - 1 and not self.shutdown_requested:
                    time.sleep(SCRAPE_CONFIG['delay_between_pages'])
            
            # Calculate final statistics
            end_time = datetime.now()
            duration = (end_time - self.start_time).total_seconds()
            
            final_stats = {
                **self.stats,
                'duration_seconds': duration,
                'start_time': self.start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'urls_processed': len(results),
                'success_rate': (self.stats['pages_scraped'] / len(results)) * 100 if results else 0
            }
            
            self.logger.info(f"ğŸ“Š Scraping session completed:")
            self.logger.info(f"   âœ… Pages scraped: {self.stats['pages_scraped']}")
            self.logger.info(f"   âŒ Pages failed: {self.stats['pages_failed']}")
            self.logger.info(f"   ğŸ“ Total content: {self.stats['total_content_length']} chars")
            self.logger.info(f"   â±ï¸  Duration: {duration:.1f} seconds")
            self.logger.info(f"   ğŸ“ˆ Success rate: {final_stats['success_rate']:.1f}%")
            
            # Save final status
            self.save_status('completed', final_stats)
            
            # Return success if at least one page was scraped
            return self.stats['pages_scraped'] > 0
            
        except Exception as e:
            error_msg = f"Critical error in scraping session: {e}"
            self.logger.error(error_msg, exc_info=True)
            self.save_status('failed', {
                'error': error_msg,
                'traceback': traceback.format_exc()
            })
            return False
    
    def cleanup(self):
        """Cleanup resources and log final status"""
        try:
            self.logger.info("ğŸ§¹ Cleaning up resources...")
            
            # Archive old logs if they're too large
            self.archive_large_logs()
            
            self.logger.info("âœ… Cleanup completed")
            
        except Exception as e:
            self.logger.error(f"Error during cleanup: {e}")
    
    def archive_large_logs(self):
        """Archive log files if they become too large"""
        max_log_size = 10 * 1024 * 1024  # 10MB
        
        for log_file in [LOG_FILE, ERROR_LOG_FILE]:
            try:
                if os.path.exists(log_file) and os.path.getsize(log_file) > max_log_size:
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    archive_name = f"{log_file}.{timestamp}"
                    os.rename(log_file, archive_name)
                    self.logger.info(f"Archived large log file: {archive_name}")
            except Exception as e:
                self.logger.error(f"Failed to archive log {log_file}: {e}")

def main():
    """Main entry point"""
    runner = SyncScraperRunner()
    
    try:
        success = runner.run_scraping_session()
        return success
        
    except KeyboardInterrupt:
        runner.logger.info("ğŸ›‘ Interrupted by user")
        return False
        
    except Exception as e:
        runner.logger.error(f"ğŸ’¥ Unexpected error: {e}", exc_info=True)
        return False
        
    finally:
        runner.cleanup()

def create_sample_config():
    """Create a sample configuration file"""
    config_file = os.path.join(os.path.dirname(__file__), 'scraper_urls.json')
    
    if not os.path.exists(config_file):
        sample_config = {
            "urls": [
                "https://news.ycombinator.com",
                "https://httpbin.org/html",
                "https://example.com",
                "https://www.reddit.com/r/programming.json"
            ],
            "config": {
                "max_pages_per_run": 10,
                "delay_between_pages": 2,
                "timeout_per_page": 30,
                "max_retries": 3
            }
        }
        
        with open(config_file, 'w') as f:
            json.dump(sample_config, f, indent=2)
        
        print(f"âœ… Created sample config file: {config_file}")

if __name__ == "__main__":
    # Handle command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == "--create-config":
            create_sample_config()
            sys.exit(0)
        elif sys.argv[1] == "--help":
            print("Usage:")
            print("  python run_scrape_sync.py                 # Run scraping session")
            print("  python run_scrape_sync.py --create-config # Create sample config")
            print("  python run_scrape_sync.py --help          # Show this help")
            sys.exit(0)
    
    # Run the scraper
    try:
        success = main()
        exit_code = 0 if success else 1
        
        print(f"\n{'âœ…' if success else 'âŒ'} Scraping session {'completed successfully' if success else 'failed'}")
        print(f"ğŸ“‹ Check logs at: {LOG_FILE}")
        print(f"ğŸ“Š Status file: {STATUS_FILE}")
        
        sys.exit(exit_code)
        
    except Exception as e:
        print(f"ğŸ’¥ Fatal error: {e}")
        sys.exit(1)
